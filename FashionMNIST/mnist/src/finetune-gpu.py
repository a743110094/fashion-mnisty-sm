"""
Fashion MNIST å¢é‡è®­ç»ƒè„šæœ¬ - åŸºäºé¢„è®­ç»ƒæ¨¡å‹
ä»ä¿å­˜çš„checkpointç»§ç»­è®­ç»ƒï¼Œæ”¯æŒè°ƒæ•´å­¦ä¹ ç‡å’Œä¼˜åŒ–ç­–ç•¥
"""

from __future__ import print_function

import argparse
import os
from datetime import datetime
from tensorboardX import SummaryWriter
from torchvision import datasets, transforms
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler

WORLD_SIZE = int(os.environ.get('WORLD_SIZE', 1))


# å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆä¸åŸå§‹ç›¸åŒï¼‰
class Net(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2)
        self.bn1 = nn.BatchNorm2d(64)

        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(128)

        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(256)

        self.dropout2d = nn.Dropout2d(p=dropout_rate)
        self.gap = nn.AdaptiveAvgPool2d((1, 1))

        self.fc1 = nn.Linear(256, 256)
        self.bn_fc = nn.BatchNorm1d(256)
        self.dropout = nn.Dropout(p=dropout_rate)
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.max_pool2d(x, 2, 2)

        x = F.relu(self.bn2(self.conv2(x)))
        x = F.max_pool2d(x, 2, 2)

        x = F.relu(self.bn3(self.conv3(x)))
        x = F.max_pool2d(x, 2, 2)

        x = self.gap(x)
        x = x.view(x.size(0), -1)

        x = self.fc1(x)
        x = self.bn_fc(x)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


def train(args, model, device, train_loader, test_loader, optimizer, epoch, writer, early_stopping):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    train_loss = 0
    num_batches = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        train_loss += loss.item()
        num_batches += 1

        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tloss={:.4f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))

            niter = epoch * len(train_loader) + batch_idx
            writer.add_scalar('loss', loss.item(), niter)

        if args.val_interval > 0 and batch_idx > 0 and batch_idx % args.val_interval == 0:
            print(f'\n=== ä¸­é—´éªŒè¯ (Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}) ===')
            val_acc, val_loss = test(args, model, device, test_loader, writer, epoch, batch_idx)

            if early_stopping.check_and_save(val_loss, val_acc, model):
                print(f'ğŸ’¾ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯ç²¾åº¦: {val_acc:.4f})')

            model.train()

    avg_train_loss = train_loss / num_batches
    return avg_train_loss


def test(args, model, device, test_loader, writer, epoch, batch_idx=None):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item()
            pred = output.max(1, keepdim=True)[1]
            correct += pred.eq(target.view_as(pred)).sum().item()

    val_loss = test_loss / len(test_loader.dataset)
    accuracy = float(correct) / len(test_loader.dataset)
    print('\nValidation Loss: {:.4f}, accuracy={:.4f}\n'.format(val_loss, accuracy))

    if batch_idx is not None:
        step = epoch * 10000 + batch_idx
        writer.add_scalar('accuracy_intra_epoch', accuracy, step)
        writer.add_scalar('val_loss_intra_epoch', val_loss, step)
    else:
        writer.add_scalar('accuracy', accuracy, epoch)
        writer.add_scalar('val_loss', val_loss, epoch)

    return accuracy, val_loss


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=10, verbose=False, delta=0.0001):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.counter = 0
        self.best_val_loss = None
        self.best_val_acc = None
        self.best_model_state = None
        self.early_stop = False
        self.val_count = 0

    def check_and_save(self, val_loss, val_acc, model):
        self.val_count += 1
        saved = False

        if self.best_val_loss is None:
            self.best_val_loss = val_loss
            self.best_val_acc = val_acc
            self.best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            saved = True
            if self.verbose:
                print(f'âœ… åˆå§‹åŒ–æœ€ä½³æ¨¡å‹ (æŸå¤±: {val_loss:.4f}, ç²¾åº¦: {val_acc:.4f})')
        elif val_loss < self.best_val_loss - self.delta:
            self.best_val_loss = val_loss
            self.best_val_acc = val_acc
            self.best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            self.counter = 0
            saved = True
            if self.verbose:
                print(f'âœ… éªŒè¯æŸå¤±é™ä½åˆ° {val_loss:.4f} (ç²¾åº¦: {val_acc:.4f})')
        else:
            self.counter += 1
            if self.verbose:
                print(f'âš ï¸  è¿ç»­ {self.counter}/{self.patience} æ¬¡éªŒè¯æ— æ”¹è¿› (æœ€ä½³æŸå¤±: {self.best_val_loss:.4f})')

        return saved

    def __call__(self, val_loss, val_acc, model):
        self.check_and_save(val_loss, val_acc, model)

        if self.counter >= self.patience:
            self.early_stop = True
            if self.verbose:
                print(f'ğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ {self.counter} æ¬¡éªŒè¯æ— æ”¹è¿›')

        return self.early_stop


def create_scheduler(optimizer, args):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    if args.scheduler == 'none':
        return None
    elif args.scheduler == 'step':
        scheduler = lr_scheduler.StepLR(optimizer, step_size=args.scheduler_step, gamma=args.scheduler_gamma)
    elif args.scheduler == 'exponential':
        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=args.scheduler_gamma)
    elif args.scheduler == 'cosine':
        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=args.min_lr)
    elif args.scheduler == 'linear':
        scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=args.epochs)
    elif args.scheduler == 'plateau':
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, factor=0.95)
    else:
        return None

    return scheduler


def main():
    # ========== 1. è§£æå‘½ä»¤è¡Œå‚æ•° ==========
    parser = argparse.ArgumentParser(description='PyTorch MNIST Finetune Example')
    parser.add_argument('--batch-size', type=int, default=256, metavar='N',
                        help='input batch size for training (default: 256)')
    parser.add_argument('--test-batch-size', type=int, default=10000, metavar='N',
                        help='input batch size for testing (default: 10000)')
    parser.add_argument('--epochs', type=int, default=50, metavar='N',
                        help='number of epochs to train (default: 50)')
    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',
                        help='learning rate for finetune (default: 0.001) - æ¯”åˆå§‹å­¦ä¹ ç‡å°å¾—å¤š')
    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',
                        help='SGD momentum (default: 0.9)')
    parser.add_argument('--scheduler', type=str, default='cosine', metavar='S',
                        choices=['none', 'step', 'exponential', 'cosine', 'linear', 'plateau'],
                        help='learning rate scheduler (default: cosine)')
    parser.add_argument('--scheduler-step', type=int, default=10, metavar='N',
                        help='step size for StepLR scheduler (default: 10)')
    parser.add_argument('--scheduler-gamma', type=float, default=0.5, metavar='G',
                        help='gamma for StepLR/ExponentialLR scheduler (default: 0.5)')
    parser.add_argument('--min-lr', type=float, default=0.00001, metavar='LR',
                        help='minimum learning rate (default: 1e-5)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables GPU training')
    parser.add_argument('--no-mps', action='store_true', default=False,
                        help='disables Mac GPU training')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
                        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=40, metavar='N',
                        help='how many batches to wait before logging')
    parser.add_argument('--val-interval', type=int, default=0, metavar='N',
                        help='how many batches to wait before validation (0 = only at end of epoch)')
    parser.add_argument('--save-model', action='store_true', default=True,
                        help='For Saving the current Model')
    parser.add_argument('--dataset', type=str, default='../data', help='dataset directory')
    parser.add_argument('--save-model-dir', type=str, default='../data/mnt', help='save directory')
    parser.add_argument('--model-path', type=str, required=True, metavar='PATH',
                        help='path to pretrained model (required)')
    parser.add_argument('--dir', default='logs_finetune', metavar='L',
                        help='directory where summary logs are stored')
    parser.add_argument('--dropout', type=float, default=0.3, metavar='D',
                        help='dropout rate (default: 0.3)')
    parser.add_argument('--freeze-conv', action='store_true', default=False,
                        help='freeze convolutional layers, only finetune FC layers')
    parser.add_argument('--freeze-bn', action='store_true', default=False,
                        help='freeze batch norm layers')

    args = parser.parse_args()

    # ========== 2. æ£€æŸ¥GPUè®¾å¤‡ ==========
    use_cuda = False
    use_mps = False

    if not args.no_cuda:
        if torch.cuda.is_available():
            use_cuda = True
            print('Using CUDA')
        elif torch.backends.mps.is_available() and not args.no_mps:
            use_mps = True
            print('Using Mac GPU (MPS)')
        else:
            print('GPU not available, using CPU')
    else:
        print('GPU disabled, using CPU')

    # ========== 3. åˆå§‹åŒ–TensorBoard ==========
    writer = SummaryWriter(args.dir)

    # ========== 4. è®¾ç½®éšæœºç§å­ ==========
    torch.manual_seed(args.seed)

    # ========== 5. è®¾ç½®è®¡ç®—è®¾å¤‡ ==========
    if use_cuda:
        device = torch.device("cuda")
    elif use_mps:
        device = torch.device("mps")
    else:
        device = torch.device("cpu")

    # ========== 6. æ•°æ®åŠ è½½å™¨é…ç½® ==========
    if use_cuda:
        kwargs = {'num_workers': 8, 'pin_memory': True, 'persistent_workers': True}
    elif use_mps:
        kwargs = {'num_workers': 6, 'pin_memory': False, 'persistent_workers': True}
    else:
        kwargs = {'num_workers': 2}

    # è®­ç»ƒé›†æ•°æ®åŠ è½½å™¨ï¼ˆè½»åº¦æ•°æ®å¢å¼ºï¼Œé¿å…è¿‡åº¦å¢å¼ºï¼‰
    train_loader = torch.utils.data.DataLoader(
        datasets.FashionMNIST(args.dataset, train=True, download=True,
                    transform=transforms.Compose([
                        transforms.RandomCrop(28, padding=2),  # è½»åº¦è£å‰ª
                        transforms.RandomHorizontalFlip(),
                        transforms.RandomRotation(10),  # è½»åº¦æ—‹è½¬
                        transforms.ToTensor(),
                        transforms.Normalize((0.1307,), (0.3081,))
                    ])),
        batch_size=args.batch_size, shuffle=True, **kwargs)

    # æµ‹è¯•é›†æ•°æ®åŠ è½½å™¨
    test_loader = torch.utils.data.DataLoader(
        datasets.FashionMNIST(args.dataset, train=False, transform=transforms.Compose([
                        transforms.ToTensor(),
                        transforms.Normalize((0.1307,), (0.3081,))
                    ])),
        batch_size=args.test_batch_size, shuffle=False, **kwargs)

    # ========== 7. åˆ›å»ºæ¨¡å‹ ==========
    model = Net(dropout_rate=args.dropout).to(device)

    # ========== 8. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ ==========
    if not os.path.exists(args.model_path):
        print(f'âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}')
        return

    print(f'ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {args.model_path}')
    checkpoint = torch.load(args.model_path, map_location=device)
    model.load_state_dict(checkpoint)
    print('âœ… æ¨¡å‹åŠ è½½æˆåŠŸ')

    # ========== 9. å†»ç»“éƒ¨åˆ†å±‚ï¼ˆå¯é€‰ï¼‰ ==========
    if args.freeze_conv:
        print('ğŸ”’ å†»ç»“å·ç§¯å±‚ï¼Œåªå¾®è°ƒFCå±‚')
        for name, param in model.named_parameters():
            if 'conv' in name or 'bn' in name:
                param.requires_grad = False
            else:
                param.requires_grad = True

    if args.freeze_bn:
        print('ğŸ”’ å†»ç»“BatchNormå±‚')
        for module in model.modules():
            if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):
                module.eval()  # è®¾ç½®ä¸ºevalæ¨¡å¼ï¼Œä¸æ›´æ–°ç»Ÿè®¡ä¿¡æ¯

    # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f'ğŸ“Š æ€»å‚æ•°: {total_params:,}, å¯è®­ç»ƒå‚æ•°: {trainable_params:,}')

    # ========== 10. åˆå§‹åŒ–ä¼˜åŒ–å™¨ ==========
    # æ³¨æ„ï¼šä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒ
    optimizer = torch.optim.AdamW(
        [p for p in model.parameters() if p.requires_grad],
        lr=args.lr,
        weight_decay=1e-2
    )

    # ========== 11. åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨ ==========
    scheduler = create_scheduler(optimizer, args)
    if scheduler is not None:
        print(f'ä½¿ç”¨ {args.scheduler} è°ƒåº¦å™¨')
    else:
        print('ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå›ºå®šå­¦ä¹ ç‡')

    # ========== 12. åˆå§‹åŒ–æ—©åœæœºåˆ¶ ==========
    early_stopping = EarlyStopping(patience=15, verbose=True, delta=0.0001)

    # ========== 13. è®­ç»ƒå¾ªç¯ ==========
    print("========== å¼€å§‹å¢é‡è®­ç»ƒ ==========")
    print(f"é¢„è®­ç»ƒæ¨¡å‹: {args.model_path}")
    print(f"å­¦ä¹ ç‡: {args.lr} (å¾®è°ƒç”¨ - è¾ƒå°)")
    print(f"Batch Size: {args.batch_size}")
    print(f"Epochs: {args.epochs}")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%y-%m-%d %H:%M:%S')}")
    print("=" * 40)

    for epoch in range(1, args.epochs + 1):
        train_loss = train(args, model, device, train_loader, test_loader, optimizer, epoch, writer, early_stopping)

        print(f'\n=== Epoch {epoch} ç»“æŸéªŒè¯ ===')
        val_acc, val_loss = test(args, model, device, test_loader, writer, epoch)

        writer.add_scalar('train_loss_avg', train_loss, epoch)

        current_lrs = [group['lr'] for group in optimizer.param_groups]
        if len(current_lrs) == 1:
            print('Epoch {} learning rate: {:.6f}'.format(epoch, current_lrs[0]))
            writer.add_scalar('learning_rate', current_lrs[0], epoch)
        else:
            lr_values = ', '.join(['{:.6f}'.format(lr) for lr in current_lrs])
            print('Epoch {} learning rates: {}'.format(epoch, lr_values))
            for index, lr in enumerate(current_lrs):
                writer.add_scalar('learning_rate/group_{}'.format(index), lr, epoch)

        # æ›´æ–°å­¦ä¹ ç‡
        if scheduler is not None:
            if args.scheduler == 'plateau':
                scheduler.step(val_loss)
            else:
                scheduler.step()

        # æ£€æŸ¥æ—©åœæ¡ä»¶
        if early_stopping(val_loss, val_acc, model):
            print(f'\n{"="*40}')
            print(f'ğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨epoch {epoch}åœæ­¢è®­ç»ƒ')
            print(f'æœ€ä½³éªŒè¯æŸå¤±: {early_stopping.best_val_loss:.4f}')
            print(f'æœ€ä½³éªŒè¯ç²¾åº¦: {early_stopping.best_val_acc:.4f}')
            print(f'{"="*40}\n')
            break

    print("========== è®­ç»ƒå®Œæˆ ==========")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%y-%m-%d %H:%M:%S')}")

    # ========== 14. ä¿å­˜æ¨¡å‹ ==========
    if args.save_model:
        if not os.path.exists(args.save_model_dir):
            os.makedirs(args.save_model_dir)

        if early_stopping.best_model_state is not None:
            print(f'\nğŸ’¾ ä¿å­˜æœ€ä½³å¾®è°ƒæ¨¡å‹')
            print(f'   éªŒè¯æŸå¤±: {early_stopping.best_val_loss:.4f}')
            print(f'   éªŒè¯ç²¾åº¦: {early_stopping.best_val_acc:.4f}')
            torch.save(early_stopping.best_model_state,
                      os.path.join(args.save_model_dir, "mnist_cnn_finetuned.pt"))
        else:
            print(f'\nğŸ’¾ ä¿å­˜å½“å‰æ¨¡å‹')
            torch.save(model.state_dict(),
                      os.path.join(args.save_model_dir, "mnist_cnn_finetuned.pt"))

    writer.close()


if __name__ == '__main__':
    main()
