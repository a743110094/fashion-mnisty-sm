"""
Fashion MNIST è®­ç»ƒè„šæœ¬ - æ”¯æŒå¤šç§GPUåç«¯
æ”¯æŒè®¾å¤‡ï¼šCUDA (NVIDIA GPU) > MPS (Mac GPU) > CPU
ä½œè€…å¤‡æ³¨ï¼šMac GPU(MPS)éœ€è¦PyTorch>=1.12.0ï¼Œä¸”M1/M2/M3ç­‰Apple SiliconèŠ¯ç‰‡
"""

from __future__ import print_function

import argparse  # å‘½ä»¤è¡Œå‚æ•°è§£æ
import os
from datetime import datetime  # æ—¶é—´æˆ³è®°å½•
from tensorboardX import SummaryWriter  # TensorBoard å¯è§†åŒ–æ—¥å¿—
from torchvision import datasets, transforms  # å›¾åƒæ•°æ®é›†å’Œå˜æ¢
import torch
import torch.distributed as dist  # åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
import torch.nn as nn  # ç¥ç»ç½‘ç»œæ¨¡å—
import torch.nn.functional as F  # ç¥ç»ç½‘ç»œå‡½æ•°ï¼ˆæ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ç­‰ï¼‰
import torch.optim as optim  # ä¼˜åŒ–å™¨
import torch.optim.lr_scheduler as lr_scheduler  # å­¦ä¹ ç‡è°ƒåº¦å™¨

# è·å–åˆ†å¸ƒå¼è®­ç»ƒçš„å…¨å±€è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º 1ï¼ˆå•æœºè®­ç»ƒï¼‰
WORLD_SIZE = int(os.environ.get('WORLD_SIZE', 1))


# å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºå›¾åƒåˆ†ç±»
class Net(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super(Net, self).__init__()
        # ç¬¬ä¸€å±‚å·ç§¯ï¼šè¾“å…¥é€šé“=1(ç°åº¦å›¾), è¾“å‡ºé€šé“=32, å·ç§¯æ ¸=5x5, æ­¥é•¿=1
        self.conv1 = nn.Conv2d(1, 64, 5, 1)
        # ç¬¬ä¸€å±‚æ‰¹é‡å½’ä¸€åŒ–ï¼ˆå·ç§¯å±‚åï¼‰
        self.bn1 = nn.BatchNorm2d(64)

        # ç¬¬äºŒå±‚å·ç§¯ï¼šè¾“å…¥é€šé“=64, è¾“å‡ºé€šé“=128, å·ç§¯æ ¸=3x3, æ­¥é•¿=1
        self.conv2 = nn.Conv2d(64, 128, 3, 1)
        # ç¬¬äºŒå±‚æ‰¹é‡å½’ä¸€åŒ–ï¼ˆå·ç§¯å±‚åï¼‰
        self.bn2 = nn.BatchNorm2d(128)

        # ç¬¬ä¸‰å±‚å·ç§¯ï¼šè¾“å…¥é€šé“=128, è¾“å‡ºé€šé“=256, å·ç§¯æ ¸=3x3, æ­¥é•¿=1
        self.conv3 = nn.Conv2d(128, 256, 3, 1)
        # ç¬¬ä¸‰å±‚æ‰¹é‡å½’ä¸€åŒ–ï¼ˆå·ç§¯å±‚åï¼‰
        self.bn3 = nn.BatchNorm2d(256)

        # Dropoutå±‚ï¼šåœ¨å·ç§¯å±‚ååº”ç”¨2D dropout
        self.dropout2d = nn.Dropout2d(p=dropout_rate)

        # ç¬¬ä¸€å±‚å…¨è¿æ¥å±‚ï¼šè¾“å…¥=256*1*1=256ï¼ˆç»è¿‡å¤šæ¬¡æ± åŒ–åï¼‰
        # è®¡ç®—è¿‡ç¨‹ï¼š28x28 -> 24x24(conv1) -> 12x12(pool) -> 10x10(conv2) -> 5x5(pool)
        #        -> 3x3(conv3) -> 1x1(pool) -> fc1(256ç»´)
        self.fc1 = nn.Linear(256*1*1, 256)
        # å…¨è¿æ¥å±‚æ‰¹é‡å½’ä¸€åŒ–
        self.bn_fc = nn.BatchNorm1d(256)

        # Dropoutå±‚ï¼šåœ¨å…¨è¿æ¥å±‚ååº”ç”¨dropout
        self.dropout = nn.Dropout(p=dropout_rate)

        # è¾“å‡ºå±‚ï¼šè¾“å…¥=256, è¾“å‡º=10ï¼ˆ10ä¸ªè¡£æœåˆ†ç±»ï¼‰
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        # ç¬¬ä¸€ä¸ªå·ç§¯å—ï¼šå·ç§¯ -> BatchNorm -> ReLUæ¿€æ´» -> æœ€å¤§æ± åŒ–(2x2) -> Dropout
        x = self.conv1(x)  # 28x28 -> 24x24
        x = self.bn1(x)  # æ‰¹é‡å½’ä¸€åŒ–
        x = F.relu(x)  # ReLUæ¿€æ´»
        x = F.max_pool2d(x, 2, 2)  # 24x24 -> 12x12
        x = self.dropout2d(x)  # åº”ç”¨2D dropoutï¼ˆæ± åŒ–åï¼‰

        # ç¬¬äºŒä¸ªå·ç§¯å—ï¼šå·ç§¯ -> BatchNorm -> ReLUæ¿€æ´» -> æœ€å¤§æ± åŒ–(2x2) -> Dropout
        x = self.conv2(x)  # 12x12 -> 10x10
        x = self.bn2(x)  # æ‰¹é‡å½’ä¸€åŒ–
        x = F.relu(x)  # ReLUæ¿€æ´»
        x = F.max_pool2d(x, 2, 2)  # 10x10 -> 5x5
        x = self.dropout2d(x)  # åº”ç”¨2D dropoutï¼ˆæ± åŒ–åï¼‰

        # ç¬¬ä¸‰ä¸ªå·ç§¯å—ï¼šå·ç§¯ -> BatchNorm -> ReLUæ¿€æ´» -> æœ€å¤§æ± åŒ–(2x2) -> Dropout
        x = self.conv3(x)  # 5x5 -> 3x3
        x = self.bn3(x)  # æ‰¹é‡å½’ä¸€åŒ–
        x = F.relu(x)  # ReLUæ¿€æ´»
        x = F.max_pool2d(x, 2, 2)  # 3x3 -> 1x1
        x = self.dropout2d(x)  # åº”ç”¨2D dropoutï¼ˆæ± åŒ–åï¼‰

        # å±•å¹³ä¸ºä¸€ç»´å‘é‡ï¼š(batch_size, 256*1*1)
        x = x.view(-1, 256*1*1)

        # å…¨è¿æ¥å±‚ -> BatchNorm -> ReLUæ¿€æ´» -> Dropout
        x = self.fc1(x)
        x = self.bn_fc(x)  # æ‰¹é‡å½’ä¸€åŒ–
        x = F.relu(x)  # ReLUæ¿€æ´»
        x = self.dropout(x)  # åº”ç”¨dropoutï¼ˆéšè—å±‚åï¼‰

        # è¾“å‡ºå±‚ï¼ˆä¸å¸¦æ¿€æ´»ã€ä¸å¸¦dropoutï¼‰
        x = self.fc2(x)

        # è¿”å›log_softmaxç”¨äºNLLæŸå¤±å‡½æ•°
        return F.log_softmax(x, dim=1)

# è®­ç»ƒå‡½æ•°ï¼šå¯¹ä¸€ä¸ªepochçš„æ•°æ®è¿›è¡Œè®­ç»ƒ
def train(args, model, device, train_loader, optimizer, epoch, writer):
    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨dropoutã€batchnormç­‰ï¼‰
    model.train()

    # ç´¯è®¡è®­ç»ƒæŸå¤±
    train_loss = 0
    num_batches = 0

    # éå†è®­ç»ƒé›†ä¸­çš„æ¯ä¸€ä¸ªbatch
    for batch_idx, (data, target) in enumerate(train_loader):
        # å°†æ•°æ®ç§»åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
        data, target = data.to(device), target.to(device)

        # æ¸…ç©ºæ¢¯åº¦ï¼ˆé˜²æ­¢æ¢¯åº¦ç´¯ç§¯ï¼‰
        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ¨¡å‹è¾“å‡º
        output = model(data)

        # è®¡ç®—æŸå¤±å‡½æ•°ï¼ˆNegative Log Likelihood Lossï¼‰
        loss = F.nll_loss(output, target)

        # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆåœ¨Mac MPSä¸Šç‰¹åˆ«é‡è¦ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # æ›´æ–°å‚æ•°
        optimizer.step()

        # ç´¯è®¡æŸå¤±
        train_loss += loss.item()
        num_batches += 1

        # æ¯éš”log_intervalä¸ªbatchæ‰“å°ä¸€æ¬¡è®­ç»ƒè¿›åº¦
        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tloss={:.4f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))

            # è®°å½•æŸå¤±åˆ°TensorBoard
            niter = epoch * len(train_loader) + batch_idx
            writer.add_scalar('loss', loss.item(), niter)

    # è®¡ç®—è¯¥epochçš„å¹³å‡è®­ç»ƒæŸå¤±
    avg_train_loss = train_loss / num_batches
    return avg_train_loss

# æµ‹è¯•å‡½æ•°ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
def test(args, model, device, test_loader, writer, epoch):
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨dropoutã€batchnormç­‰ï¼‰
    model.eval()

    # åˆå§‹åŒ–æµ‹è¯•æŸå¤±å’Œæ­£ç¡®æ•°
    test_loss = 0
    correct = 0

    # åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆèŠ‚çœå†…å­˜å’Œè®¡ç®—é‡ï¼‰
    with torch.no_grad():
        # éå†æµ‹è¯•é›†ä¸­çš„æ¯ä¸€ä¸ªbatch
        for data, target in test_loader:
            # å°†æ•°æ®ç§»åˆ°æŒ‡å®šè®¾å¤‡
            data, target = data.to(device), target.to(device)

            # å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ¨¡å‹è¾“å‡º
            output = model(data)

            # ç´¯è®¡æµ‹è¯•æŸå¤±ï¼ˆä½¿ç”¨sumè€Œä¸æ˜¯meanï¼Œæœ€åå†é™¤ä»¥æ€»æ ·æœ¬æ•°ï¼‰
            test_loss += F.nll_loss(output, target, reduction='sum').item()

            # è·å–é¢„æµ‹ç»“æœï¼šå–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ç´¢å¼•
            pred = output.max(1, keepdim=True)[1]

            # ç´¯è®¡æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°
            correct += pred.eq(target.view_as(pred)).sum().item()

    # è®¡ç®—å¹³å‡æµ‹è¯•æŸå¤±ï¼ˆéªŒè¯é›†æŸå¤±ï¼‰
    val_loss = test_loss / len(test_loader.dataset)

    # è®¡ç®—å¹¶æ‰“å°å‡†ç¡®ç‡
    accuracy = float(correct) / len(test_loader.dataset)
    print('\nValidation Loss: {:.4f}, accuracy={:.4f}\n'.format(val_loss, accuracy))

    # è®°å½•å‡†ç¡®ç‡å’ŒéªŒè¯æŸå¤±åˆ°TensorBoard
    writer.add_scalar('accuracy', accuracy, epoch)
    writer.add_scalar('val_loss', val_loss, epoch)

    # è¿”å›éªŒè¯ç²¾åº¦å’ŒæŸå¤±ï¼Œä¾›schedulerå’Œæ—©åœæœºåˆ¶ä½¿ç”¨
    return accuracy, val_loss


# æ—©åœæœºåˆ¶ç±»
class EarlyStopping:
    """
    ç›‘å¬éªŒè¯ç²¾åº¦ï¼Œå¦‚æœè¿ç»­patienceä¸ªepochæ²¡æœ‰æ”¹è¿›ï¼Œåˆ™åœæ­¢è®­ç»ƒ
    åŒæ—¶ä¿å­˜æœ€ä½³æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    """
    def __init__(self, patience=10, verbose=False, delta=0.0001):
        """
        Args:
            patience (int): è¿ç»­å¤šå°‘ä¸ªepochæ²¡æœ‰æ”¹è¿›ååœæ­¢è®­ç»ƒï¼Œé»˜è®¤10
            verbose (bool): æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œé»˜è®¤False
            delta (float): æœ€å°æ”¹è¿›é˜ˆå€¼ï¼Œç²¾åº¦æå‡å°äºdeltaè§†ä¸ºæ²¡æœ‰æ”¹è¿›
        """
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.counter = 0  # è®°å½•æ²¡æœ‰æ”¹è¿›çš„epochæ¬¡æ•°
        self.best_val_acc = None  # è®°å½•æœ€ä½³éªŒè¯ç²¾åº¦
        self.best_model_state = None  # ä¿å­˜æœ€ä½³æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        self.best_epoch = None  # è®°å½•æœ€ä½³æ¨¡å‹å‡ºç°çš„epoch
        self.early_stop = False  # æ˜¯å¦åœæ­¢è®­ç»ƒçš„æ ‡å¿—

    def __call__(self, val_acc, model):
        """
        æ£€æŸ¥éªŒè¯ç²¾åº¦æ˜¯å¦æ”¹è¿›ï¼Œå¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
        Args:
            val_acc (float): å½“å‰epochçš„éªŒè¯ç²¾åº¦
            model: å½“å‰çš„æ¨¡å‹å¯¹è±¡
        Returns:
            bool: æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
        """
        if self.best_val_acc is None:
            # ç¬¬ä¸€ä¸ªepochï¼Œè®°å½•æœ€ä½³ç²¾åº¦å’Œæ¨¡å‹
            self.best_val_acc = val_acc
            self.best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}
            self.best_epoch = 1
        elif val_acc > self.best_val_acc + self.delta:
            # ç²¾åº¦æœ‰æ”¹è¿›ï¼Œä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
            self.best_val_acc = val_acc
            self.best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}
            self.counter = 0  # é‡ç½®è®¡æ•°å™¨
            if self.verbose:
                print(f'âœ… Validation accuracy improved to {val_acc:.4f}')
        else:
            # ç²¾åº¦æ²¡æœ‰æ”¹è¿›
            self.counter += 1
            if self.verbose:
                print(f'âš ï¸  No improvement for {self.counter}/{self.patience} epochs (best: {self.best_val_acc:.4f})')

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f'ğŸ›‘ Early stopping triggered after {self.counter} epochs without improvement')

        return self.early_stop


# åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
def create_scheduler(optimizer, args):
    """
    æ ¹æ®å‚æ•°åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨

    æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹ï¼š
    - 'none': ä¸ä½¿ç”¨è°ƒåº¦å™¨ï¼Œå­¦ä¹ ç‡ä¿æŒä¸å˜
    - 'step': æ¯éš”å›ºå®šæ­¥æ•°é™ä½å­¦ä¹ ç‡ (StepLR)
    - 'exponential': æŒ‰æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ (ExponentialLR)
    - 'cosine': ä½¿ç”¨ä½™å¼¦å‡½æ•°è°ƒåº¦å­¦ä¹ ç‡ (CosineAnnealingLR)
    - 'linear': çº¿æ€§è¡°å‡å­¦ä¹ ç‡ (LinearLR)
    - 'plateau': å½“éªŒè¯æŸå¤±åœæ­¢æ”¹è¿›æ—¶é™ä½å­¦ä¹ ç‡ (ReduceLROnPlateau) - æ¨èï¼

    æ³¨æ„ï¼šReduceLROnPlateauéœ€è¦åœ¨æ¯ä¸ªepochä¼ å…¥éªŒè¯æŸå¤±ï¼šscheduler.step(val_loss)
    å…¶ä»–è°ƒåº¦å™¨åªéœ€è°ƒç”¨ï¼šscheduler.step()

    Args:
        optimizer: PyTorch ä¼˜åŒ–å™¨å®ä¾‹
        args: åŒ…å«è°ƒåº¦å™¨å‚æ•°çš„å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡

    Returns:
        scheduler å¯¹è±¡æˆ– None
    """
    if args.scheduler == 'none':
        return None
    elif args.scheduler == 'step':
        # StepLR: æ¯ step_size ä¸ª epoch ä¹˜ä»¥ gamma
        # ä¾‹å¦‚ï¼šgamma=0.1, step_size=10 è¡¨ç¤ºæ¯10ä¸ªepochå­¦ä¹ ç‡ä¹˜ä»¥0.1
        scheduler = lr_scheduler.StepLR(optimizer, step_size=args.scheduler_step, gamma=args.scheduler_gamma)
    elif args.scheduler == 'exponential':
        # ExponentialLR: æ¯ä¸ª epoch ä¹˜ä»¥ gamma
        # ä¾‹å¦‚ï¼šgamma=0.95 è¡¨ç¤ºæ¯ä¸ªepochå­¦ä¹ ç‡ä¹˜ä»¥0.95
        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=args.scheduler_gamma)
    elif args.scheduler == 'cosine':
        # CosineAnnealingLR: ä½¿ç”¨ä½™å¼¦å‡½æ•°ä»åˆå§‹LRè¡°å‡åˆ°æœ€å°LR
        # T_max æ˜¯å‘¨æœŸï¼ˆepochæ•°ï¼‰ï¼Œafter T_max ä¸ª epoch å­¦ä¹ ç‡å°†è¡°å‡åˆ°æœ€å°å€¼
        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=args.min_lr)
    elif args.scheduler == 'linear':
        # LinearLR: çº¿æ€§è¡°å‡å­¦ä¹ ç‡
        # total_iters æ˜¯æ€»è¿­ä»£æ¬¡æ•°
        scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=args.epochs)
    elif args.scheduler == 'plateau':
        # ReduceLROnPlateau: å½“éªŒè¯é›†çš„å‡†ç¡®ç‡æ²¡æœ‰æå‡æ—¶ï¼Œå°†å­¦ä¹ ç‡è¡°å‡
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.8)
    else:
        return None

    return scheduler


# åˆ¤æ–­æ˜¯å¦åº”è¯¥å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
def should_distribute():
    # æ£€æŸ¥åˆ†å¸ƒå¼è®­ç»ƒæ˜¯å¦å¯ç”¨ä¸”è¿›ç¨‹æ•°å¤§äº1
    return dist.is_available() and WORLD_SIZE > 1


# åˆ¤æ–­åˆ†å¸ƒå¼è®­ç»ƒæ˜¯å¦å·²åˆå§‹åŒ–
def is_distributed():
    # æ£€æŸ¥åˆ†å¸ƒå¼è®­ç»ƒæ˜¯å¦å¯ç”¨ä¸”å·²åˆå§‹åŒ–
    return dist.is_available() and dist.is_initialized()


# ä¸»å‡½æ•°ï¼šè®­ç»ƒæµç¨‹çš„å…¥å£
def main():
    # ========== 1. è§£æå‘½ä»¤è¡Œå‚æ•° ==========
    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
    parser.add_argument('--batch-size', type=int, default=50, metavar='N',
                        help='input batch size for training (default: 128)')
    parser.add_argument('--test-batch-size', type=int, default=500, metavar='N',
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=2000, metavar='N',
                        help='number of epochs to train (default: 50)')
    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',
                        help='learning rate (default: 0.01)')
    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',
                        help='SGD momentum (default: 0.9) - æ³¨ï¼šä½¿ç”¨Adamä¼˜åŒ–å™¨æ—¶æ­¤å‚æ•°ä¸èµ·ä½œç”¨')
    parser.add_argument('--scheduler', type=str, default='cosine', metavar='S',
                        choices=['none', 'step', 'exponential', 'cosine', 'linear', 'plateau'],
                        help='learning rate scheduler: none, step, exponential, cosine, linear, plateau (default: cosine)')
    parser.add_argument('--scheduler-step', type=int, default=10, metavar='N',
                        help='step size for StepLR scheduler (fault: 30)')
    parser.add_argument('--scheduler-gamma', type=float, default=0.5, metavar='G',
                        help='gamma for StepLR/ExponentialLR scheduler (default: 0.02)')
    parser.add_argument('--min-lr', type=float, default=1e-4, metavar='LR',
                        help='minimum learning rate for cosine annealing scheduler (default: 1e-4)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables GPU training (CUDA and MPS)')
    parser.add_argument('--no-mps', action='store_true', default=False,
                        help='disables Mac GPU (MPS) training, use CPU instead')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
                        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=40, metavar='N',
                        help='how many batches to wait before logging training status')
    parser.add_argument('--save-model', action='store_true', default=True,
                        help='For Saving the current Model')
    parser.add_argument('--dataset', type=str, default='../data', help='For dataset director')

    parser.add_argument('--save-model-dir', type=str, default='../data/mnt', help='For Saving directory')

    parser.add_argument('--dir', default='logs', metavar='L',
                        help='directory where summary logs are stored')
    parser.add_argument('--dropout', type=float, default=0.2, metavar='D',
                        help='dropout rate (default: 0.2)')
    if dist.is_available():
        parser.add_argument('--backend', type=str, help='Distributed backend',
                            choices=[dist.Backend.GLOO, dist.Backend.NCCL, dist.Backend.MPI],
                            default=dist.Backend.GLOO)
    args = parser.parse_args()

    # ========== 2. æ£€æŸ¥å¹¶è®¾ç½®GPUè®¾å¤‡ ==========
    # ä¼˜å…ˆçº§ï¼šCUDA > MPS (Mac) > CPU
    use_cuda = False
    use_mps = False

    if not args.no_cuda:
        if torch.cuda.is_available():
            use_cuda = True
            print('Using CUDA')
        elif torch.backends.mps.is_available() and not args.no_mps:
            # Mac GPU æ”¯æŒ (Metal Performance Shaders)
            use_mps = True
            print('Using Mac GPU (MPS)')
        else:
            if torch.backends.mps.is_available() and args.no_mps:
                print('Mac GPU (MPS) is available but disabled by --no-mps flag')
            else:
                print('GPU not available, using CPU')
    else:
        print('GPU disabled by --no-cuda flag, using CPU')

    # ========== 3. åˆå§‹åŒ–TensorBoardæ—¥å¿—å†™å…¥å™¨ ==========
    writer = SummaryWriter(args.dir)

    # ========== 4. è®¾ç½®éšæœºç§å­ï¼ˆä¿è¯å¯é‡å¤æ€§ï¼‰ ==========
    torch.manual_seed(args.seed)

    # ========== 5. è®¾ç½®è®¡ç®—è®¾å¤‡ ==========
    # æ ¹æ®å¯ç”¨çš„GPUç±»å‹é€‰æ‹©è®¾å¤‡
    if use_cuda:
        device = torch.device("cuda")
    elif use_mps:
        device = torch.device("mps")
    else:
        device = torch.device("cpu")

    # ========== 6. åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰ ==========
    if should_distribute():
        print('Using distributed PyTorch with {} backend'.format(args.backend))
        dist.init_process_group(backend=args.backend)

    # ========== 7. æ•°æ®åŠ è½½å™¨é…ç½® ==========
    # æ ¹æ®è®¾å¤‡ç±»å‹é…ç½®æ•°æ®åŠ è½½å‚æ•°
    # CUDA: å¯ç”¨å¤šè¿›ç¨‹å’Œå†…å­˜é”å®šä»¥åŠ é€Ÿæ•°æ®ä¼ è¾“
    # MPS: ä¸ä½¿ç”¨pin_memoryï¼ˆMac GPUä¸æ”¯æŒï¼‰ï¼Œä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
    # CPU: åŸºç¡€å¤šè¿›ç¨‹é…ç½®
    if use_cuda:
        kwargs = {'num_workers': 64, 'pin_memory': True, 'persistent_workers': True}
    elif use_mps:
        # Mac GPU ä¸æ”¯æŒ pin_memoryï¼Œä½†å¯ä»¥ç”¨å¤šè¿›ç¨‹åŠ é€Ÿæ•°æ®åŠ è½½
        kwargs = {'num_workers': 6, 'pin_memory': False, 'persistent_workers': True}
    else:
        # CPU æ¨¡å¼
        kwargs = {'num_workers': 2}

    # è®­ç»ƒé›†æ•°æ®åŠ è½½å™¨ï¼ˆå«æ•°æ®å¢å¼ºï¼‰
    # - FashionMNISTæ•°æ®é›†è‡ªåŠ¨ä¸‹è½½
    # - åº”ç”¨æ•°æ®å¢å¼ºï¼šéšæœºæ—‹è½¬ã€åç§»ã€äº®åº¦è°ƒæ•´
    # - è½¬æ¢ä¸ºå¼ é‡å¹¶è¿›è¡Œæ ‡å‡†åŒ–
    # - æ‰“ä¹±æ•°æ®ä»¥å¢åŠ æ³›åŒ–èƒ½åŠ›
    train_loader = torch.utils.data.DataLoader(
        datasets.FashionMNIST(args.dataset, train=True, download=True,
                    transform=transforms.Compose([
                        transforms.RandomCrop(28, padding=4),  # éšæœºè£å‰ªä¿æŒå°ºå¯¸
                        transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬
                        transforms.RandomRotation(10),  # éšæœºæ—‹è½¬Â±10åº¦
                        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # éšæœºå¹³ç§»Â±10%
                        transforms.ColorJitter(brightness=0.2),  # éšæœºäº®åº¦è°ƒæ•´Â±20%
                        transforms.ToTensor(),
                        transforms.Normalize((0.1307,), (0.3081,))
                    ])),
        batch_size=args.batch_size, shuffle=True, **kwargs)

    # æµ‹è¯•é›†æ•°æ®åŠ è½½å™¨
    # - ä¸ä¸‹è½½ï¼ˆå¦‚æœå·²å­˜åœ¨ï¼‰
    # - ä¸æ‰“ä¹±æ•°æ®
    test_loader = torch.utils.data.DataLoader(
        datasets.FashionMNIST(args.dataset, train=False, transform=transforms.Compose([
                        transforms.ToTensor(),
                        transforms.Normalize((0.1307,), (0.3081,))
                    ])),
        batch_size=args.test_batch_size, shuffle=False, **kwargs)

    # ========== 8. åˆ›å»ºæ¨¡å‹å¹¶ç§»åˆ°è®¾å¤‡ ==========
    model = Net(dropout_rate=args.dropout).to(device)

    # ========== 9. å¯ç”¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆå¦‚æœéœ€è¦ï¼‰ ==========
    # æ³¨æ„ï¼šMac MPS ç›®å‰ä¸æ”¯æŒ DistributedDataParallelï¼Œä»…æ”¯æŒå•æœºè®­ç»ƒ
    if is_distributed():
        if use_mps:
            print('Warning: Mac GPU (MPS) does not support DistributedDataParallel yet.')
            print('Using single-machine training only.')
        else:
            Distributor = nn.parallel.DistributedDataParallel if use_cuda else nn.parallel.DistributedDataParallelCPU
            model = Distributor(model)

    # ========== 10. åˆå§‹åŒ–ä¼˜åŒ–å™¨ ==========
    # ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œç‰¹åˆ«é€‚åˆæ·±å±‚ç½‘ç»œ
    # Adam = Adaptive Moment Estimationï¼Œç»“åˆäº†momentumå’ŒRMSpropçš„ä¼˜ç‚¹
    # weight_decay=1e-4 æ˜¯L2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢æƒé‡è¿‡å¤§ï¼ˆè¿‡æ‹Ÿåˆï¼‰
    #optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=1e-4)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)

    # ========== 10.5. åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨ ==========
    # æ ¹æ®å‚æ•°åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
    scheduler = create_scheduler(optimizer, args)
    if scheduler is not None:
        print('Using {} scheduler'.format(args.scheduler))
    else:
        print('No learning rate scheduler, using fixed learning rate')

    # ========== 11. åˆå§‹åŒ–æ—©åœæœºåˆ¶ ==========
    early_stopping = EarlyStopping(patience=40, verbose=True, delta=0.0001)

    # ========== 12. è®­ç»ƒå¾ªç¯ï¼ˆå«æ—©åœï¼‰ ==========
    print("begin training: ", datetime.now().strftime('%y-%m-%d %H:%M:%S'))
    for epoch in range(1, args.epochs + 1):
        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè·å–å¹³å‡è®­ç»ƒæŸå¤±
        train_loss = train(args, model, device, train_loader, optimizer, epoch, writer)

        # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè·å–éªŒè¯ç²¾åº¦å’ŒæŸå¤±
        val_acc, val_loss = test(args, model, device, test_loader, writer, epoch)

        # è®°å½•å¹³å‡è®­ç»ƒæŸå¤±åˆ°TensorBoard
        writer.add_scalar('train_loss_avg', train_loss, epoch)

        current_lrs = [group['lr'] for group in optimizer.param_groups]
        if len(current_lrs) == 1:
            print('Epoch {} learning rate: {:.6f}'.format(epoch, current_lrs[0]))
            writer.add_scalar('learning_rate', current_lrs[0], epoch)
        else:
            lr_values = ', '.join(['{:.6f}'.format(lr) for lr in current_lrs])
            print('Epoch {} learning rates: {}'.format(epoch, lr_values))
            for index, lr in enumerate(current_lrs):
                writer.add_scalar('learning_rate/group_{}'.format(index), lr, epoch)

        # åœ¨æ¯ä¸ªepochåæ›´æ–°å­¦ä¹ ç‡ï¼ˆå¦‚æœä½¿ç”¨äº†è°ƒåº¦å™¨ï¼‰
        if scheduler is not None:
            # ReduceLROnPlateauéœ€è¦ä¼ å…¥éªŒè¯æŸå¤±ï¼Œå…¶ä»–è°ƒåº¦å™¨ä¸éœ€è¦
            if args.scheduler == 'plateau':
                scheduler.step(val_loss)
            else:
                scheduler.step()

        # æ£€æŸ¥æ—©åœæ¡ä»¶ï¼ˆä¼ å…¥modelç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€ï¼‰
        if early_stopping(val_acc, model):
            print(f'\n{"="*60}')
            print(f'ğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨epoch {epoch}åœæ­¢è®­ç»ƒ')
            print(f'æœ€ä½³éªŒè¯ç²¾åº¦: {early_stopping.best_val_acc:.4f}')
            print(f'è¿ç»­{early_stopping.counter}ä¸ªepochæ²¡æœ‰æ”¹è¿›')
            print(f'{"="*60}\n')
            break

    print("end training: ", datetime.now().strftime('%y-%m-%d %H:%M:%S'))

    # ========== 13. ä¿å­˜æ¨¡å‹ ==========
    if (args.save_model):
        if not os.path.exists(args.save_model_dir):
            os.makedirs(args.save_model_dir)

        # å¦‚æœä½¿ç”¨äº†æ—©åœæœºåˆ¶ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹ï¼›å¦åˆ™ä¿å­˜æœ€åçš„æ¨¡å‹
        if early_stopping.best_model_state is not None:
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæ—©åœè§¦å‘æˆ–æ­£å¸¸è®­ç»ƒå®Œæˆï¼‰
            print(f'ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯ç²¾åº¦: {early_stopping.best_val_acc:.4f}ï¼‰')
            torch.save(early_stopping.best_model_state, os.path.join(args.save_model_dir, "mnist_cnn.pt"))
        else:
            # é™çº§æ–¹æ¡ˆï¼šä¿å­˜å½“å‰æ¨¡å‹çŠ¶æ€
            print(f'ğŸ’¾ ä¿å­˜å½“å‰æ¨¡å‹')
            torch.save(model.state_dict(), os.path.join(args.save_model_dir, "mnist_cnn.pt"))

if __name__ == '__main__':
    main()
